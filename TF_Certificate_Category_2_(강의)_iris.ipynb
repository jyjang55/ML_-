{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyjang55/ML_DeepLearning_Of_All/blob/master/TF_Certificate_Category_2_(%EA%B0%95%EC%9D%98)_iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY-Ca0dhGTAk"
      },
      "source": [
        "# Category 2 - Iris 꽃 분류\n",
        "\n",
        "* Fully Connected Layer (Dense)를 활용한 분류 모델 (Classification)\n",
        "* tensorflow-datasets 를 활용한 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww0hxcaSGb0m"
      },
      "source": [
        "## 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI6oK1X-GdJJ"
      },
      "source": [
        "1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yoe-K38GQ4b"
      },
      "source": [
        "## 순서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc6iTV8DGPop"
      },
      "source": [
        "1. **import**: 필요한 모듈 import\n",
        "2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.\n",
        "3. **모델링(model)**: 모델을 정의합니다.\n",
        "4. **컴파일(compile)**: 모델을 생성합니다.\n",
        "5. **학습 (fit)**: 모델을 학습시킵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msi1agesayxW"
      },
      "source": [
        "## 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2BLvK-5ayxW"
      },
      "source": [
        "For this task you will train a classifier for Iris flowers using the Iris dataset\n",
        "\n",
        "The final layer in your neural network should look like: tf.keras.layers.\n",
        "\n",
        "Dense(3, activation=tf.nn.softmax)\n",
        "\n",
        "The input layer will expect data in the shape (4,)\n",
        "\n",
        "We've given you some starter code for preprocessing the data\n",
        "\n",
        "You'll need to implement the preprocess function for data.map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4eg-t_payxX"
      },
      "source": [
        "\n",
        "이 작업에서는 Iris 데이터 세트를 사용하여 Iris 꽃 분류기를 훈련시킵니다.\n",
        "\n",
        "신경망의 마지막 계층은 tf.keras.layers와 같아야합니다.\n",
        "\n",
        "**Dense(3, activation='softmax')**\n",
        "\n",
        "입력 레이어는 모양의 데이터를 기대합니다 (4,)\n",
        "\n",
        "데이터 전처리를위한 스타터 코드를 제공했습니다\n",
        "\n",
        "data.map에 대한 전처리 기능을 구현해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WdRZMk2ngyR"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trHhF2j1njgc"
      },
      "source": [
        "Image('https://user-images.githubusercontent.com/15958325/56006707-f69f3680-5d10-11e9-8609-25ba5034607e.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-3xqy_ga4KO"
      },
      "source": [
        "## import 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj9EzELCa8FN"
      },
      "source": [
        "필요한 모듈을 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9M_SARJnil5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HGYCJ1Tnil7"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg0sXFW2nil7"
      },
      "source": [
        "**tensorflow-datasets**를 활용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTjsZFWGn117"
      },
      "source": [
        "* [Iris 데이터셋 문서 보기](https://www.tensorflow.org/datasets/catalog/iris?hl=ko)\n",
        "\n",
        "* [tensorflow-datasets 다루기](https://www.tensorflow.org/datasets/splits?hl=ko)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTCJ8FxgogB_"
      },
      "source": [
        "**시험에서 주어지는 데이터셋 로드 형태**\n",
        "\n",
        "* 예전 방식이므로 아래처러 주어지는 코드를 과감히 삭제 후, 아래 제공되는 방식으로 변경합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWsdj7Dgnil8"
      },
      "source": [
        "data = tfds.load(\"iris\", split=tfds.Split.TRAIN.subsplit(tfds.percent[:80]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C4BAqkeo6Oz"
      },
      "source": [
        "`train_dataset`와 `valid_dataset`을 만들고 **80% : 20%로 분할**합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLww60Tro2uP"
      },
      "source": [
        "train_dataset = tfds.load('iris', split='train[:80%]')\n",
        "valid_dataset = tfds.load('iris', split='train[80%:]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km-ZwVjjpQsW"
      },
      "source": [
        "## tensorflow-datasets 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42v_2b2pUTU"
      },
      "source": [
        "`train_dataset`를 출력해 보면, 디테일한 정보를 나타내지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvaHFrRgpEwN"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9s07QTlpgU3"
      },
      "source": [
        "먼저, 시험에서는 다음과 같은 **전처리 요구 조건**이 있습니다.\n",
        "\n",
        "1. label 값을 **one-hot encoding** 할 것\n",
        "2. feature (x), label (y)를 분할할 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgRnvtDJpEj0"
      },
      "source": [
        "**[실습코드]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNuRAHVGpHdK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqskpv7qsF4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csLdvlgWsFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXTEKQ2AsFuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Um7KmS0pHRN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvWmrFxHsmGz"
      },
      "source": [
        "for data in train_dataset.take(1):\n",
        "    print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PrXvnpspHNy"
      },
      "source": [
        "def preprocess(data):\n",
        "    # 코드를 입력하세요\n",
        "    x = data['features']\n",
        "    y = data['label']\n",
        "    y = tf.one_hot(y, 3)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cga_qgeXqgCT"
      },
      "source": [
        "**batch**란?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEC86Y1qftY"
      },
      "source": [
        "1개의 Epoch에서 **여러 개의 샘플을 나누어 학습하는 단위** 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAT3Hdseq5hM"
      },
      "source": [
        "**예시**\n",
        "\n",
        "**1,000개의 이미지**를 학습하는 경우.\n",
        "\n",
        "* **batch_size=10** => (1000개 이미지) / (batch_size=10) = 총 **100개의 batch**\n",
        "* **batch_size=20** => (1000개 이미지) / (batch_size=20) = 총 **50개의 batch**\n",
        "* **batch_size=50** => (1000개 이미지) / (batch_size=50) = 총 **20개의 batch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPQ9hOrnpHKj"
      },
      "source": [
        "만든 전처리 함수(preprocessing)를 **dataset에 mapping**하고, **batch_size도 지정**합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-xytZurjnb"
      },
      "source": [
        "batch_size=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO5cJujCqSyJ"
      },
      "source": [
        "train_data = train_dataset.map(preprocess).batch(batch_size)\n",
        "valid_data = valid_dataset.map(preprocess).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6B3UzpUnim4"
      },
      "source": [
        "## 모델 정의 (Sequential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azRvYVbQnim5"
      },
      "source": [
        "이제 Modeling을 할 차례입니다.\n",
        "\n",
        "`Sequential` 모델 안에서 층을 깊게 쌓아 올려 주면 됩니다.\n",
        "\n",
        "1. `input_shape`는 Iris 꽃 데이터셋의 X의 feature 갯수가 4개 이므로 **(4, )**로 지정합니다.\n",
        "2. 깊은 출력층과 더 많은 Layer를 쌓습니다.\n",
        "3. Dense Layer에 `activation='relu'`를 적용합니다.\n",
        "4. 분류(Classification)의 마지막 층의 출력 숫자는 분류하고자 하는 클래스 갯수와 **같아야** 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCDinGHSnim5"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # input_shape는 X의 feature 갯수가 4개 이므로 (4, )로 지정합니다.\n",
        "    Dense(512, activation='relu', input_shape=(4,)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    # Classification을 위한 Softmax, 클래스 갯수 = 3개\n",
        "    Dense(3, activation='softmax'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u4rp7tB-4A1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnmQV-fDnim8"
      },
      "source": [
        "## 컴파일 (compile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHQ1abHXK8e9"
      },
      "source": [
        "1. `optimizer`는 가장 최적화가 잘되는 알고리즘인 'adam'을 사용합니다.\n",
        "2. `loss`설정\n",
        "  * 출력층 activation이 `sigmoid` 인 경우: `binary_crossentropy`\n",
        "  * 출력층 activation이 `softmax` 인 경우: \n",
        "    * 원핫인코딩(O): `categorical_crossentropy`\n",
        "    * 원핫인코딩(X): `sparse_categorical_crossentropy`)\n",
        "3. `metrics`를 'acc' 혹은 'accuracy'로 지정하면, 학습시 정확도를 모니터링 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNurLvnisN-t"
      },
      "source": [
        "전처리 단계에서 **one-hot encoding** 을 해주었습니다. 따라서, `categorical_crossentropy`를 지정해주면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXGMljJQhmp_"
      },
      "source": [
        "model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCLw6RMZnim-"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyLUPgGCninB"
      },
      "source": [
        "## ModelCheckpoint: 체크포인트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Oi04ZMLtEB"
      },
      "source": [
        "`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.\n",
        "* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.\n",
        "* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJwGq3PoninB"
      },
      "source": [
        "checkpoint_path = \"my_checkpoint.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_weights_only=True, \n",
        "                             save_best_only=True, \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3mjb5EAninE"
      },
      "source": [
        "## 학습 (fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-X6hK_DMYZH"
      },
      "source": [
        "1. `validation_data`를 반드시 지정합니다.\n",
        "2. `epochs`을 적절하게 지정합니다.\n",
        "3. `callbacks`에 바로 위에서 만든 checkpoint를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uHXDA_vninF"
      },
      "source": [
        "history = model.fit(train_data,\n",
        "                    validation_data=(valid_data),\n",
        "                    epochs=20,\n",
        "                    callbacks=[checkpoint],\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnduSgRiBw8"
      },
      "source": [
        "## 학습 완료 후 Load Weights (ModelCheckpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLqb_6XrMvdq"
      },
      "source": [
        "학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.\n",
        "\n",
        "그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jO1ucZ9ninH"
      },
      "source": [
        "# checkpoint 를 저장한 파일명을 입력합니다.\n",
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cG8SGdninJ"
      },
      "source": [
        "## 학습 Loss (오차)에 대한 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrsnoy8otDAk"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_xoc7AlninJ"
      },
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(np.arange(1, 21), history.history['loss'])\n",
        "plt.plot(np.arange(1, 21), history.history['val_loss'])\n",
        "plt.title('Loss / Val Loss', fontsize=20)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['loss', 'val_loss'], fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iSgnD3fninN"
      },
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(np.arange(1, 21), history.history['acc'])\n",
        "plt.plot(np.arange(1, 21), history.history['val_acc'])\n",
        "plt.title('Acc / Val Acc', fontsize=20)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Acc')\n",
        "plt.legend(['acc', 'val_acc'], fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTAnUue0tB4O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}